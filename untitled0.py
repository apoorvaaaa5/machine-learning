# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IAXxPc0nLKjYthlA4H7zS7auIw18DE0d
"""

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(missing_values=np.nan,strategy='mean')
imputer.fit(X[:, 1:3])
X[:, 1:3]=imputer.transform(X[:, 1:3])
from sklearn.compose import ColumnTransformer
from sklearn.prepocessing import OneHotEncoder
from sklearn.transformer(transformers=[('encoder',OneHotEncoder(),[0])].remainder='passthrough') #[0]-to avoid encoding of same name like spain in different ways and hence want to code in same way
x=np..array(ct.fit_transform(X))
#encoding the dependent variable
from sklearn.preprocessing import LabelEncoder
le=

#splittingthe dataset into the training set and test set
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)
print(X_train)

#feature scaling
from sklear,preprocessingimport StandardScaler
sc=StandardScaler()
X_train[:, 3:]=sc.fit_transform(X_train[:, 3:])
X_test[:, 3:]=sc.fit_transform(X_test[:, 3:])
print(X_train)

#7-11-2022
#unsupervised
#superised--> 1)regression(numbers)--> if(y value>50)then classify data into this category or other [under supervised we have regression and classification]
#2)classification same asregression but the last if statement is not gonna be there
#[y_test y_predict] for predicting the test set results
#m means slope vlaue should be same for linear regression
#but for parabola the slope is different for other points henceit is not linear whereas it is polynomial regression
# multiple regression is where you compare different inputs which is from one input to another all possibilities and even compare each input with output is multiple regression
#line of classification defines if it is above or below this then it is this means it classifies
#svm ,decision tree and random forest ---
#support vectors-it is goin to find eqn to find distance btw gutter and gutter itself
#boundary points which lieson gutter are called support vectors
#svm is very very imp and the only disadvantage is when outliers is there,,,,svm is very helpful for small data
#kernels it is to bring shape to smtg
#rbf